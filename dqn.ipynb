{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 深度强化学习 DQN 实验\n",
    "\n",
    "实验采用 OpenAI Gymnasium 的 Frozen Lake 环境\n",
    "\n",
    "![frozen_lake](imgs/frozen_lake.gif)"
   ],
   "id": "2aa8af84e641c45a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import collections\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 训练设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "ad3e3e00b0196815",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 环境\n",
    "\n",
    "环境为一个 4x4 的网格世界，Agent 可以往上下左右四个方向移动。Agent 从起点出发，需要避开冰湖到达礼物的位置。\n",
    "\n",
    "状态空间为：$\\mathbf{S} = \\{0,1,...,15\\}$\n",
    "\n",
    "动作空间为：$\\mathbf{A} = \\{0,1,2,3\\}$"
   ],
   "id": "ec92ebe2895cd971"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 网格世界各网格可能的状态 token\n",
    "BLANK = 0\n",
    "LAKE = 1\n",
    "GIFT = 2\n",
    "PLAYER = 3\n",
    "\n",
    "# 网格世界地图\n",
    "grid = torch.tensor([\n",
    "    [BLANK, BLANK, BLANK, BLANK],\n",
    "    [BLANK, LAKE, BLANK, LAKE],\n",
    "    [BLANK, BLANK, BLANK, LAKE],\n",
    "    [LAKE, BLANK, BLANK, GIFT]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# 将玩家 token 放置在地图上\n",
    "def player_on_grid(state):\n",
    "    batch_size = state.size(0)\n",
    "    ret = grid.unsqueeze(0).repeat(batch_size, 1, 1).to(state.device)\n",
    "    indices = torch.cat((state.unsqueeze(-1) // 4, state.unsqueeze(-1) % 4), dim=-1)\n",
    "    for i in range(batch_size):\n",
    "        ret[i, indices[i,0], indices[i,1]] = PLAYER\n",
    "    return ret"
   ],
   "id": "91c46e68adeafa0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 模型结构\n",
    "\n",
    "模型采用一个嵌入层和三层卷积网络提取特征，然后通过两个线性层估计Q值。\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 模型训练\n",
    "\n",
    "训练过程分为两部分，首先需要从环境中获取训练样本，称为经验回放，存在经验回放缓冲器中，然后根据经验回放训练模型。其中获取经验回放需要使用 Epsilon-Greedy 算法。\n",
    "\n",
    "<br>\n",
    "\n",
    "**从环境中采样过程如下**：\n",
    "\n",
    "首先使用 Epsilon-Greedy 获取动作：\n",
    "$$\n",
    "x \\sim \\text{Uniform}[0,1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "a = \\underset{a}{\\text{argmax}} Q(s,a), & x \\gt \\epsilon \\\\\n",
    "a \\sim \\text{Uniform}(\\mathbf{A}), & x \\le \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "然后将动作应用到环境，可以获取一个五元组 $(s_t,a_t,r_t,s_{t+1},\\text{done})$ 。将这个元组存储到经验回放缓冲器，直到数量达到缓冲器的最大数量。\n",
    "\n",
    "<br>\n",
    "\n",
    "**训练过程如下**：\n",
    "\n",
    "从经验回放缓冲器中随机采样一批数据，并计算 loss：\n",
    "$$\n",
    "\\text{target} = r_t + \\gamma \\underset{a_{t+1}}{\\text{max}} Q(s_{t+1},a_{t+1})\n",
    "$$\n",
    "其中 $\\gamma$ 是折扣因子，\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\text{MSELoss}(Q(s_t,a_t), \\text{target})\n",
    "$$\n",
    "\n",
    "然后就可以对 loss 进行反向传播啦。"
   ],
   "id": "246f0263af16eaa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 深度Q网络模型\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size=(4, 4), action_size=4, grid_states=4, d_model=16, lr=1e-3):\n",
    "        super().__init__()\n",
    "        # 模型结构参数\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.grid_states = grid_states\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 深度网络\n",
    "        self.emb = nn.Embedding(self.grid_states, self.d_model)\n",
    "        self.featnet = nn.Sequential(\n",
    "            nn.Conv2d(d_model, 2*d_model, kernel_size=3, stride=1, padding=1),          # (4,4,16) -> (4,4,32)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2*d_model, 4*d_model, kernel_size=3, stride=1, padding=1),        # (4,4,32) -> (4,4,64)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4*d_model, 8*d_model, kernel_size=3, stride=1),                   # (4,4,64) -> (2,2,128)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.vnet = nn.Sequential(\n",
    "            nn.Linear(self._feat_size(), self._feat_size() // 8),                        # (512) -> (64)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self._feat_size() // 8, self.action_size)                          # (64) -> (4)\n",
    "        )\n",
    "        \n",
    "        # 采用均方差损失函数\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # 迭代相关的数据\n",
    "        self.iterations = 0\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def _feat_size(self):\n",
    "        return 8 * (self.state_size[0] - 2) * (self.state_size[1] - 2) * self.d_model\n",
    "        \n",
    "    # 前向过程\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size = x.size(0)\n",
    "        x = player_on_grid(x)\n",
    "        x = self.emb(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.featnet(x)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        x = self.vnet(x)\n",
    "        return x\n",
    "\n",
    "    # 采样\n",
    "    def act(self, x: int, eps=0.1):\n",
    "        # Epsilon-Greedy\n",
    "        if random.random() > eps:\n",
    "            with torch.no_grad():\n",
    "                out = self.forward(torch.tensor([x], device=device))\n",
    "                return torch.argmax(out, dim=-1).squeeze().cpu().item()\n",
    "        else:\n",
    "            return random.randint(0, self.action_size-1)\n",
    "\n",
    "    # 训练\n",
    "    def update(self, sample, gamma=0.9):\n",
    "        state, action, reward, next_state, done = sample\n",
    "        # tuple 转 tensor\n",
    "        state = torch.tensor(state, device=device)\n",
    "        action = torch.tensor(action, device=device)\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "        next_state = torch.tensor(next_state, device=device)\n",
    "        done = torch.tensor(done, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # 训练主要逻辑\n",
    "        self.optimizer.zero_grad()\n",
    "        max_next_q = torch.max(self.forward(next_state), dim=1)[0] * (~done)\n",
    "        target = reward + gamma * max_next_q\n",
    "        q = self.forward(state).gather(1, action[:,None])[:,0]\n",
    "        loss = self.criterion(q, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.iterations += 1\n",
    "        return loss.item()"
   ],
   "id": "b864aa6315533a9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 经验回放缓冲器\n",
    "class ExpReplayBuffer:\n",
    "    def __init__(self, max_len=1024):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = collections.deque(maxlen=max_len)     # 使用队列存储\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        \n",
    "    def is_full(self):\n",
    "        return len(self.buffer) == self.max_len\n",
    "    \n",
    "    def append(self, sample):\n",
    "        self.buffer.append(sample)\n",
    "    \n",
    "    # 随机取出一定样本用于模型训练\n",
    "    def sample(self, n=256):\n",
    "        zipped = random.sample(list(self.buffer), n)\n",
    "        return list(zip(*zipped))"
   ],
   "id": "46f0ef326ee285ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 从环境中采样数据用于训练模型\n",
    "def sample_from_env(env: gym.Env, net: nn.Module, buffer: ExpReplayBuffer):\n",
    "    net.eval()\n",
    "    obs = 0\n",
    "    env_running = False\n",
    "    buffer.clear()\n",
    "    while not buffer.is_full():\n",
    "        if not env_running:\n",
    "            obs, info = env.reset()\n",
    "            env_running = True\n",
    "        action = net.act(obs)\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action)\n",
    "        env_running = not (terminated or truncated)\n",
    "        buffer.append((obs, action, reward, obs_next, terminated))\n",
    "        obs = obs_next\n",
    "    env.close()\n",
    "\n",
    "# 训练模型\n",
    "def train_model(net: nn.Module, buffer: ExpReplayBuffer, batch_size=256):\n",
    "    samples = buffer.sample(batch_size)\n",
    "    net.train()\n",
    "    return net.update(samples, gamma=0.9)"
   ],
   "id": "e991baad6f45f34b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 推理，无图形化渲染，用于采集训练时的模型拟合情况\n",
    "def infer(env: gym.Env, net: nn.Module):\n",
    "    net.eval()\n",
    "    obs, info = env.reset()\n",
    "    episode_over = False\n",
    "    steps = 0\n",
    "    reward_cnt = 0\n",
    "    while not episode_over:\n",
    "        action = net.act(obs, eps=0.0)\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "        obs = obs_next\n",
    "        steps += 1\n",
    "        reward_cnt += reward\n",
    "    env.close()\n",
    "    return reward_cnt, steps\n",
    "\n",
    "# 推理，图形化渲染，用于直观地观察模型的运动\n",
    "def infer_human_render(env: gym.Env, net: nn.Module):\n",
    "    net.eval()\n",
    "    obs, info = env.reset()\n",
    "    env.render()\n",
    "    episode_over = False\n",
    "    steps = 0\n",
    "    reward_cnt = 0\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    while not episode_over:\n",
    "        action = net.act(obs, eps=0.0)\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "        obs = obs_next\n",
    "        steps += 1\n",
    "        reward_cnt += reward\n",
    "    env.close()\n",
    "    if reward_cnt == 0:\n",
    "        print(f'模型未到达终点，共计{steps}步')\n",
    "    else:\n",
    "        print(f'模型到达终点，共计{steps}步')"
   ],
   "id": "e1b84d7a65d3b6cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 保存模型\n",
    "def save_model(net: DQN):\n",
    "    state_dict = {\n",
    "        'iterations': net.iterations,\n",
    "        'model': net.state_dict(),\n",
    "        'optimizer': net.optimizer.state_dict(),\n",
    "    }\n",
    "    save_path = f'./models/dqn_{net.iterations}.pth'\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved at {save_path}.')\n",
    "\n",
    "# 加载模型\n",
    "def load_model(net: DQN, path: str):\n",
    "    state_dict = torch.load(path)\n",
    "    net.iterations = state_dict['iterations']\n",
    "    net.load_state_dict(state_dict['model'])\n",
    "    net.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "    print(f'Model loaded from {path}.')"
   ],
   "id": "97eaf83c1b2c56ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "net = DQN()\n",
    "net = net.to(device)\n",
    "buffer = ExpReplayBuffer(max_len=1024)\n",
    "\n",
    "loss_rec = []\n",
    "reward_rec = []\n",
    "steps_rec = []\n",
    "\n",
    "for i in range(1500):\n",
    "    sample_from_env(env, net, buffer)\n",
    "    loss = train_model(net, buffer, batch_size=256)\n",
    "    reward, steps = infer(env, net)\n",
    "    loss_rec.append(loss)\n",
    "    reward_rec.append(reward)\n",
    "    steps_rec.append(steps)\n",
    "    print(f'Iteration {net.iterations} finished with loss {loss}')"
   ],
   "id": "b08f67f01ede59d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_model(net)",
   "id": "fcb16dea7cce6c76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_rec)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(reward_rec)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(steps_rec)\n",
    "plt.show()"
   ],
   "id": "c8fc79dbc5d05238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3022fd84f2761a6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
