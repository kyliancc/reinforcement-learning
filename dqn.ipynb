{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 深度强化学习 DQN 实验\n",
    "\n",
    "实验采用 OpenAI Gymnasium 的 Frozen Lake 环境\n",
    "\n"
   ],
   "id": "2aa8af84e641c45a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import collections\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BLANK = 0\n",
    "LAKE = 1\n",
    "GIFT = 2\n",
    "PLAYER = 3\n",
    "    \n",
    "grid = torch.tensor([\n",
    "    [BLANK, BLANK, BLANK, BLANK],\n",
    "    [BLANK, LAKE, BLANK, LAKE],\n",
    "    [BLANK, BLANK, BLANK, LAKE],\n",
    "    [LAKE, BLANK, BLANK, GIFT]\n",
    "], dtype=torch.long)\n",
    "\n",
    "def player_on_grid(state):\n",
    "    batch_size = state.size(0)\n",
    "    ret = grid.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    indices = torch.cat((state.unsqueeze(-1) // 4, state.unsqueeze(-1) % 4), dim=-1)\n",
    "    for i in range(batch_size):\n",
    "        ret[i, indices[i,0], indices[i,1]] = PLAYER\n",
    "    return ret"
   ],
   "id": "91c46e68adeafa0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size=(4, 4), action_size=4, grid_states=4, d_model=16, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.grid_states = grid_states\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.emb = nn.Embedding(self.grid_states, self.d_model)\n",
    "        \n",
    "        self.featnet = nn.Sequential(\n",
    "            nn.Conv2d(d_model, 2*d_model, kernel_size=3, stride=1, padding=1),          # (4,4,16) -> (4,4,32)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2*d_model, 4*d_model, kernel_size=3, stride=1, padding=1),        # (4,4,32) -> (4,4,64)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4*d_model, 8*d_model, kernel_size=3, stride=1),                   # (4,4,64) -> (2,2,128)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.vnet = nn.Sequential(\n",
    "            nn.Linear(self._feat_size(), self._feat_size() // 8),                        # (512) -> (64)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self._feat_size() // 8, self.action_size)                          # (64) -> (4)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def _feat_size(self):\n",
    "        return 8 * (self.state_size[0] - 2) * (self.state_size[1] - 2) * self.d_model\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size = x.size(0)\n",
    "        x = player_on_grid(x)\n",
    "        x = self.emb(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.featnet(x)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        x = self.vnet(x)\n",
    "        return x\n",
    "    \n",
    "    def act(self, x: int, eps=0.1):\n",
    "        # Epsilon-Greedy\n",
    "        if random.random() > eps:\n",
    "            with torch.no_grad():\n",
    "                out = self.forward(torch.tensor([x]))\n",
    "                return torch.argmax(out, dim=-1).squeeze().item()\n",
    "        else:\n",
    "            return random.randint(0, self.action_size-1)\n",
    "        \n",
    "    def update(self, sample, gamma=1.0):\n",
    "        state, action, reward, next_state, done = sample\n",
    "        state = torch.tensor(state)\n",
    "        action = torch.tensor(action)\n",
    "        reward = torch.tensor(reward)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        done = torch.tensor(done, dtype=torch.bool)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        max_next_q = torch.max(self.forward(next_state), dim=1)[0] * (~done)\n",
    "        target = reward + gamma * max_next_q\n",
    "        q = self.forward(state).gather(1, action[:,None])[:,0]\n",
    "        loss = self.criterion(q, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ],
   "id": "b864aa6315533a9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ExpReplayBuffer:\n",
    "    def __init__(self, max_len=1024):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = collections.deque(maxlen=max_len)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        \n",
    "    def is_full(self):\n",
    "        return len(self.buffer) == self.max_len\n",
    "    \n",
    "    def append(self, sample):\n",
    "        self.buffer.append(sample)\n",
    "    \n",
    "    def sample(self, n=256):\n",
    "        zipped = random.sample(list(self.buffer), n)\n",
    "        return list(zip(*zipped))"
   ],
   "id": "46f0ef326ee285ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_from_env(env: gym.Env, net: nn.Module, buffer: ExpReplayBuffer):\n",
    "    net.eval()\n",
    "    obs = 0\n",
    "    env_running = False\n",
    "    buffer.clear()\n",
    "    while not buffer.is_full():\n",
    "        if not env_running:\n",
    "            obs, info = env.reset()\n",
    "            env_running = True\n",
    "        action = net.act(obs)\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action)\n",
    "        env_running = not (terminated or truncated)\n",
    "        buffer.append((obs, action, reward, obs_next, terminated))\n",
    "        obs = obs_next\n",
    "    env.close()"
   ],
   "id": "e991baad6f45f34b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(net: nn.Module, buffer: ExpReplayBuffer, batch_size=256):\n",
    "    samples = buffer.sample(batch_size)\n",
    "    net.train()\n",
    "    net.update(samples, gamma=1.0)"
   ],
   "id": "bf952cc13e25bef7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "net = DQN()\n",
    "buffer = ExpReplayBuffer(max_len=1024)\n",
    "sample_from_env(env, net, buffer)\n",
    "train_model(net, buffer, batch_size=256)"
   ],
   "id": "b08f67f01ede59d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
